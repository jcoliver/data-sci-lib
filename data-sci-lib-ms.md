---
title: Data Science Support at the Academic Library
author: Jeffrey C. Oliver, Christine Kollen, Benjamin Hickson, J. Fernando Rios
---

# Abstract

# Introduction
The interdisciplinary field of data science is emerging as one of the fastest growing endeavors on and off college campuses. Data science, which applies concepts from statistics and computer sciences to questions in numerous domains, is driving novel discoveries across disciplines. Such applications include automated linguistic analysis for "just in time" action in online support forums (Kornfield et al. 2018), neuroimage processing for predicting health outcomes (Lancaster et al. 2018), and relationships between environmental variation and childhood cognitive development (Stingone et al. 2017). However, applying such approaches requires specialized skills and resources, including computer programming and data visualization. Such skills are critical for the modern workforce (National Academies of Sciences, Engineering, and Medicine, 2018). The absence of comprehensive data science support at many academic institutions is creating a growing list of unmet needs (Oliver 2017; Barone et al. 2017; Galanek & Brooks 2018; Garcia-Milian et al. 2018), preventing many researchers from taking advantage of the big data revolution.

A number of factors are contributing to the gap in skills and resources necessary to capitalize on data science applications. The first is the relative youth of data science; while the field has arguably existed for decades (Donoho 2017), it has only recently attracted widespread attention across academic disciplines. Many researchers are only now realizing the necessity of skills development (Barone et al. 2017), and they are often in positions where formal training is especially challenging (e.g. post-doctoral or faculty positions). Also contributing to the gap is an absence of training opportunities, often due to a lack of interest in teaching "remedial" computer programming courses (Wilson 2016). When training is offered, it often suffers from "siloing" observed on many campuses, where IT departments teach to one audience, libraries teach to another, and computer science departments teach to yet another. Efforts are underway to support researchers' needs, such as Software and Data Carpentry workshops ([https://carpentries.org](https://carpentries.org)), yet there remain significant deficiencies in resources to support the development of data science literacy on many, if not most, campuses (Galanek & Brooks 2018).

Supporting data science efforts presents an opportunity for academic library engagement. Libraries serve as hubs for college and university campuses, and like the field of data science, are by definition interdisciplinary. The audience for an academic library is also arguably larger than that of any other campus unit: the library serves the _entire campus_, including students, staff, and faculty. Supporting aspects of data science in the library is not new; indeed, libraries have been supporting data collection, management, and sharing best practices for many years (Antell et al. 2014). Additionally, some academic libraries have been supporting researchers with training in computational and statistical skills (e.g. North Carolina State University: [https://www.lib.ncsu.edu/services/data-visualization](https://www.lib.ncsu.edu/services/data-visualization), New York University Libraries: [https://library.nyu.edu/departments/data-services/](https://library.nyu.edu/departments/data-services/), Arizona State University: [https://lib.asu.edu/data](https://lib.asu.edu/data)). With this in mind, now is an opportune time for academic libraries to explicitly address the growing need for data science training.

In this paper, we describe how the University of Arizona Libraries has been meeting data science needs by promoting skills and resources development. A number of approaches, including internal and external partnerships have been essential in developing a successful program. We also discuss the challenge of providing support in data science efforts, primarily in the form of personnel expertise.

# Supporting data science
The University of Arizona Libraries (UAL) has significantly expanded support for data science skills development in recent years. This includes hiring additional library staff, developing digital materials, and forming new partnerships with other intramural and extramural units. The success of these efforts is evident in growing participation in library offerings, and has relied on determining which areas of data science are appropriate for the library to support and which areas are better suited for our partners outside of the library.

## Areas of emphasis within the library
Investment in data sciences at UAL has supported growth in a number of areas, but three in particular have been especially popular on the University of Arizona campus. These heavily rely on the expertise of library staff, including deep domain knowledge in areas outside the library and information sciences. All three efforts have been successful by rapidly responding to campus needs and refining support offerings in response to the changing research landscape.

### Computational literacy
The need for computational skills is rapidly growing and is often exceeding the capacity of current training opportunities on college campuses (Wilson 2016). The University of Arizona provides weekly workshops on the popular programming language R (R Core Team 2018). These two-hour sessions alternate week-to-week between two different formats: (1) a hands-on instruction session, focusing on a specific topic such as the graphics package ggplot (Wickham 2016) or an introduction to basic statistical analyses ([https://jcoliver.github.io/learn-r](https://jcoliver.github.io/learn-r)); and (2) an open lab, where researchers bring their questions and code to work and get support in a positive, inclusive environment. The former (instruction session) are designed to provide "quick-wins", whereby researchers can immediately apply what they learn to their own work. However, they also offer the opportunity to teach best practices regarding reproducibility, abstraction, and automation. The latter format (open session) has been especially supportive of the R community of practice at the University of Arizona - workshop participants often help one another to solve challenges in data analysis and visualization. These workshops align with a number of the "Ten simple rules for biologists learning to program" (Carey & Papin 2018), including "begin with the end in mind", "develop good habits early on", and "phone a friend."

The design of R workshops has been an iterative process, with some implementations working better than others. The current workshop formats, alternating between focused instructional sessions and open lab sessions, arose after trying a number of approaches, with varying success. One approach employed early on was to offer very general workshops, introducing the R language from a very conceptual approach, with topics like variables, data types, and control flow. This approach left many participants wondering how to apply what they learned to their own work. This led to a switch to the topic-specific workshops, which are still general enough for a broad audience, and for the most part designed for novices. Another aspect of the workshops that required refinement was the duration. Initial R workshops were two full-day sessions; this was too long for many researchers, and often had considerably high attrition by the middle of the second day. Furthermore, the two-day workshop had significant duplication with Software Carpentry ([https://software-carpentry.org/](https://software-carpentry.org/)) workshops being run on the University of Arizona campus. In contrast, attempts at one-hour instruction sessions often failed to provide enough material to be useful. The two-hour sessions provide an ideal compromise between enough time to be useful and short enough duration to allow many full-time researchers to participate.

In addition to formal workshops, UAL's Data Science Specialist (an author on this work, JCO) provides in-depth consults in R data analyses and visualization, focusing largely on support in the life sciences (the Data Science Specialist's own background is in bioinformatics and ecology). The largest audience for these consults is graduate students, who often have little to no formal training in computational skills (Valle & Berdanier 2012, Rubinstein & Chor 2014). Balancing the level of involvement requires careful consideration - the library cannot offer a concierge data analyst service to every researcher on campus, but consults can provide an opportunity for true collaboration and deeper learning. Communicating clear expectations and understanding other campus resources that provide support, such as statistical consulting centers, is critical for meaningful, appropriate computational literacy support to the University of Arizona research community.

The University of Arizona Libraries also provides limited support for the Python programming language (Python Software Foundation, 2018). This limited support is largely driven by two factors. First, when computational literacy support programming was being developed, many life science researchers were requesting R support, rather than Python. Second, other campus units support Python programming, thus R programming represented a greater unmet need when the workshops were being developed. However, through conversations with R workshop participants it is increasingly evident that there remains an unmet need for Python skills development, especially in the life sciences. Should a library face the choice of offering R or Python support, assessing researchers' needs would be invaluable in making an impactful decision.

### Geographical Information Systems (GIS)
Cartography is often overlooked as one of the earliest forms of data visualization as well as the most prevalent. Visualizing spatial data on a map can fundamentally be thought of as plotting data in a well-defined domain. Spatial data differs from other data types in that its relevance, by definition, is tied to its position in the domain (where it's located on a map). This can be thought of as the difference between using a table of attributes to generate a chart (non-spatial) and using a chart (the spatial component) to generate or add to a table (spatial). With this regard, most data produced in society or obtained from our natural environments are, or can be, regarded as geospatial data because they have a geographic location component. Data can span from the field of remote sensing with the exponentially increasing stream of satellite and aerial data all the way to social media sources such as Twitter, which includes geographic location of individual tweets.

The ubiquity and enormity of geospatial information which is produced today has spawned an increase in recognition and usage in the potential for spatial analysis across and between disciplines. An increase in the recognition of geospatial data types and better access to geospatial tools has led to a subsequent increase in the need for support of spatial data.  Being able to provide support services specifically targeting geospatial data and software from a centralized campus entity like a library allows research in any college or discipline at the institution to benefit from that support.

Geospatial data support at the University of Arizona Libraries spans two broad, overlapping categories: desktop or online platforms specializing in GIS and GIS-focused application of scripting languages and large-scale computing resources. While there is a sustained demand at the University of Arizona for comprehensive and heavy desktop software platforms such as ArcGIS (ESRI 2018) and QGIS (QGIS Development Team 2018), there is prominent increase in desire to use web-based platforms such as ArcGIS Online (ESRI 2018). Because of the simplicity of access, web-based GIS has allowed for faster and broader adoptions of geospatial analysis and cartography which has allowed entire classrooms to incorporate GIS into their curriculum with little instruction. The use of online GIS platforms allows many users in the campus research environment to accomplish their cartographic and analysis needs without needing to embrace, or pay for, a fully comprehensive desktop GIS.

Given the size of datasets and increase in computational literacy in the research community, many users desire GIS support for common programming languages such as R and Python. This is especially true for GIS applications in the environmental sciences (Valle & Berdanier 2012). Knowledge of the geospatial application of programming languages is critical in big-data scenarios and virtual machine environments exploiting the power of cloud computing. The potential to address bigger research questions with larger datasets is often unrealized due to many researchers' lack of ability to use cloud computing and high performance/throughput computing resources. Such a deficit is often due to limited training opportunities for novice to intermediate users (Ye et al. 2013; Wilson 2016).

Efforts to promote and assist in broadening the skillsets of researchers in geospatial analysis have focused on offering a variety of rotating introductory workshops in one- to two-hour sessions. Providing researchers with software, data structure, and geospatial concepts in short format workshops has been more successful for the novice user base in large part because users are being introduced to languages and software for the first time and the overhead in teaching necessary background concepts can be substantial in geospatial materials. Based on attendance numbers, there is a growing interest in workshops teaching open-source software, such as QGIS, GRASS (GRASS Development Team 2017), and SAGA (Conrad et al. 2015). This is likely due to the fact that the formal curriculum for GIS at the University of Arizona is centered on the use of the ArcGIS platform, with less attention given to open source options which students are curious about. Furthermore, because ArcGIS is not supported on the macOS operating system, many open source options are platform-independent and thus are accessible to a broader audience of researchers.

At UAL, supporting more advanced analysis has typically fallen to one-on-one consultation due the customization and complexity of the particular application to the research question. Offering and scheduling one-on-ones is at the availability of expertise and convenience for the patron. In an effort to provide more informal support, weekly open drop-in hours in a centrally located lab on campus have been successful in engaging researchers. Attendance at these sessions has also facilitated collaboration between geospatial researchers on campus in disparate fields.

While support for geospatial data analysis at UAL is continuing to grow, we are continuing to adapt our outreach and support strategies while we identify and match the needs of the research body on campus. In many cases, there is still a lack of understanding at the University of Arizona campus that UAL offers  geospatial data support services. While an up-to-date website enumerating UAL options ([http://libguides.library.arizona.edu/GIS](http://libguides.library.arizona.edu/GIS)) continues to be a successful avenue to discover what the Libraries have to offer in terms of geospatial data support, developing a network in the broader communities of practice has been just as effective. This includes ensuring that other service points in the library are informed of geospatial data offerings as well as initiating and participating in frequent communication with other campus units that are providing geospatial data support.

### Reproducible science
As a cornerstone of the scientific process, the ability to verify research claims by repeating the process using one or all of the same methods, materials, data, and software, is of the utmost importance to not only maintain the credibility and reliability of research, but also to enable others to build on the work. Although there is no universally accepted definition of the term "reproducibility" (Barba 2018), it is used here to mean the ability to examine and verify the claims supported by the application of tools and methods from data science as defined in the introduction section. This of course implies that the researcher has provided enough detailed information such as how the analysis was conducted – the code, any needed computer or software settings, protocols used etc. There have been several studies in a wide variety of disciplines that have demonstrated how a large number of research studies cannot be reproduced (Ioannidis 2005; Open Science Collaboration 2015; Camerer et al. 2016).

Data science and reproducibility go hand-in-hand. That is, the proper application of data science principles and tools enables reproducibility and adopting practices that encourage reproducibility (e.g., well-documented data collection, following data management best-practices) enables good data science. For example, to move towards research reproducibility, it is important to verify that the data science tools selected are applicable to the data and the problem at hand and to remain mindful of pitfalls such as p-hacking (selective reporting of data to obtain statistical significance), and HARKing (Kerr 1998). When it comes to good data science, especially in the context of computational analyses, following best practices for the organization of data files and code, using interoperable file formats, following best practices for working with tabular data, and using literate programming all enable good data science. The final point, literate programming (Knuth 1984), which combines computer code and explanatory text in a single "living document," has become increasingly accessible through tools such as Jupyter Notebooks (Kluyver et al. 2016) and the knitr (Xie 2014) and rmarkdown (Allaire et al. 2018) R packages.

Although many academic libraries (including UAL) already have expertise to support one or more aspects of data-intensive and computational research, the knowledge is usually distributed amongst data management, geospatial, digital humanities, and data science specialists (Sayre & Riegelman 2018). The idea of personnel dedicated to reproducibility is still quite novel but the combination of reproducibility and data science support is a natural one (as exemplified in Steeves, 2017).

At UAL, support for reproducibility has taken the form of integrating best practices for data management, promotion of scripting/software to automate workflows, promotion of tools that support reproducible research (e.g., Jupyter notebooks), and advocating for open research practices into workshops and lectures. Target audiences have ranged from undergraduate students to faculty, with the exact nature of the content naturally varying depending on the audience. As an example of how research reproducibility is beginning to be integrated into instruction, a two-part workshop for postdocs dealing with topics such as data management basics, data management planning, and funder/publisher policies was revamped to include material (including hands-on activities) on best practices for data archiving, curation, and publication ([https://osf.io/td5jr](https://osf.io/td5jr)). When targeting reproducibility topics to undergraduates, the approach was to introduce the concepts of open science, transparency, and reproducibility from the point of view of being a responsible researcher, focusing less on publication and compliance with data and code sharing policies and more on hands-on skills for enabling reproducibility. These included an introduction to doing analyses using the R programming language and a hands-on activity for how to use the Open Science Framework (OSF; Spies 2013) to organize research.

Hands-on activities with the OSF have been especially popular in workshops across all audiences, with many attendees mentioning the potential utility of the platform for their own work. The UAL has also hosted workshops from the Center for Open Science (the developers of the OSF) to speak on reproducible research and the OSF. Although the number of University of Arizona-affiliated users on the OSF has increased by 40% likely as a result of these activities, combined with the fact that projects on the OSF are private by default, it is difficult to quantify if the initial enthusiasm from new users translates to sustained use. Nevertheless, examining the publicly visible projects affiliated with the University of Arizona shows there are signs that researchers are finding the OSF a useful vehicle for sharing their finished work in a more reproducible way.

The design of reproducibility workshops has been an iterative process, and  interspersing short presentations along with hands-on exercises has been the most effective format. Participants are more interested in attending workshops that meet a practical need, such as how to write a data management plan, how to find data in a data repository, or how to use the Open Science Framework. It is much easier for participants to apply knowledge and skills if they have the opportunity to try out different tools and best practices.

In addition, the UAL sponsors programs during International Open Access Week ([http://www.openaccessweek.org/](http://www.openaccessweek.org/)) and Love Data Week ([http://lovedataweek.org/](http://lovedataweek.org/)). Open Access Week programs have included issues around reproducibility, computational science, and visualizing "big data", while this year's Love Data Week program focused engaged researchers to share how data facilitates telling a story about their research.  

Although efforts to promote and enable reproducible research by the UAL are still in their infancy, continuing and expanding such efforts in conjunction with campus and external partners is an ongoing goal. These could take the form of workshops focused specifically on "doing" reproducible research using established tools, exploring new tools such as Code Ocean ([https://codeocean.com/](https://codeocean.com/)), and partnering with the University's Research Computing office to develop training for applying data management and reproducibility best practices specific to high performance computing resources.

### Promotion & advertising
A constant consideration, not unique to data science services, is that of finding the appropriate media for effectively communicating library services with the audiences that would benefit from those services. In many cases, once a critical mass has been reached, word of mouth has proven very effective in increasing participation in drop-in sessions. Colleague recommendations have driven many new users to library services, including R workshops and consultations, GIS support, and assistance with data management practices. However, reaching that critical mass can be challenging. The relatively static medium of web pages provides some details on events and services, but requires the audience to already be looking for those services on the library web page. In our experience, the best means of self-promotion has been e-mail messaging. Joining mailing lists serving key communities on campus (e.g. graduate students, GIS users, individual departments) has allowed broadcasting services to broad, yet appropriate audiences. Working with coordinators of specific majors, graduate programs, or departments has also been effective for forwarding announcements for workshops or training events. While this can add some administrative burden to library staff (i.e. sending an e-mail to each of fifteen different program coordinators), it has worked better than impersonal, _en masse_ e-mail messages. Finally, it has been critical to maintain constant communication with liaison colleagues at UAL, who are then able to promote data science support to their constituent academic units.

## Partnerships outside the library
Recognizing that no one academic unit could provide comprehensive data science support, the University of Arizona Libraries has formed several partnerships with the intent of improving data science literacy. The partnerships vary in the type and amount of resources provided by the library and include instructional space, marketing and promotion, and expertise of library staff. In terms of space, UAL has a number of contemporary learning spaces well-equipped for workshops involving in-person and remote instruction on data science tools. In collaboration with University Information Technology Services (UITS), UAL has hosted workshops on machine learning, big data analyses with Hadoop, and an introduction to computing with graphical processing units (GPUs). Workshop registrations for these popular topics routinely reach capacity within hours of being announced. Partnerships such as these highlight a growing interest in library spaces a critical contributions to data science education. As "neutral ground," outside any one department's or unit's physical space, the library serves a natural nexus for interdisciplinary collaboration for data science education (Moore-Sloan Data Science Environments 2018). Library spaces are also increasingly important for "hack"-style events, which are growing in impact and popularity in data science education (Huppenkothen et al. 2018); UAL hosts three such annual events, the Women's Hackathon ([https://womenshackathon.arizona.edu/](https://womenshackathon.arizona.edu/)), Hack Arizona ([https://new.library.arizona.edu/events/hack-arizona](https://new.library.arizona.edu/events/hack-arizona)), and the Research Bazaar ([https://resbazaz.github.io/resbaz2018/tucson/](https://resbazaz.github.io/resbaz2018/tucson/)).

To address campus-wide data science literacy, UAL partners with BIO5 ([http://www.bio5.org](http://www.bio5.org)) and the National Science Foundation's cyberinfrustructure project, CyVerse ([http://cyverse.org](http://cyverse.org), Merchant et al. 2016, 10.1371/journal.pbio.1002342) to sponsor Software and Data Carpentry workshops at the University of Arizona campus. Briefly, Carpentry workshops are intensive, two-day workshops that provide a hands-on introduction to a number of data science approaches. Rather than duplicating efforts in lesson development, all Carpentries workshops are taught from open instructional material available under a Creative Commons Attribution license. Another strength of Carpentries workshops is they are lead by volunteers who have undergone the Carpentries Instructor Training program ([https://carpentries.github.io/instructor-training/](https://carpentries.github.io/instructor-training/)), which focuses on the science of learning. UAL has recently increased the number of badged Carpentry instructors, and one of the authors of this work (JCO) is part of the Instructor Training team.

Several units across the University of Arizona offer support in data sciences, yet "siloing" hinders communication to audiences outside the core constituencies of each respective units. To address these communication gaps, and strategically develop new data science opportunities, UAL partnered with CyVerse and the Data Science Institute ([https://datascience.arizona.edu](https://datascience.arizona.edu)) to form the Data Science Resources and Training (DSRT) group. A grass-roots organization, the DSRT steering committee advocates for data science education and oversees a clearinghouse of training events and resources at the University of Arizona ([https://datascience.arizona.edu/dsrt](https://datascience.arizona.edu/dsrt)). The clearinghouse uses a controlled vocabulary of data science topics, allowing researchers to identify those opportunities relevant to their work. The diversity of resources enumerated on the DSRT clearinghouse website illustrates the importance of interdisciplinary collaboration in developing a data-literate campus. As more data science centers and institutes are established at institutions of higher learning, the academic library has a valuable opportunity to collaborate towards improved information sharing and elevating campus data science training resources.

The UAL also partners with the Office of Research, Development and Innovation (RDI) in a number of different capacities to facilitate campus data literacy. Each semester, UAL teaches a data management workshop, at the request of RDI Responsible Conduct of Research Program. RDI Research Development Services refers researchers to the UAL for assistance writing data management plans as part of grant proposals. The Public Access Working Group composed of representatives from the UAL and RDI developed a website to provide guidance on complying with policies for public access for federally funded research ([http://new.library.arizona.edu/research/funder-requirements](http://new.library.arizona.edu/research/funder-requirements)) and made campus presentations related to federal agency changes as a result of the 2013 Office of Science and Technology Policy’s "Increasing Access to the Results of Federally Funded Scientific Research" Memorandum ([https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/ostp_public_access_memo_2013.pdf](https://obamawhitehouse.archives.gov/sites/default/files/microsites/ostp/ostp_public_access_memo_2013.pdf)). These partnerships with RDI illustrate how academic libraries' work with research offices can help support a culture of open science across campus.

# Personnel needs
Developing data science support at an academic library is predicated on having library staff qualified to provide support. Generally speaking there are two means of expanding expertise: training existing personnel and hiring staff anew. Both present challenges, including fiscal and cultural considerations.

The first option for developing expertise at a library is through training of current staff. This is being made increasingly easier through the growing number of opportunities for librarians to develop data science skills. For novice audiences, the Library Carpentry workshops ([http://librarycarpentry.org/](http://librarycarpentry.org/), Baker et al. 2016) provide an introduction to data and analytical skills. These face-to-face workshops are generally two-day events that include hands-on lessons on topics such as the Unix shell, version control with Git, or the Python programming language. The University of Arizona hosted Library Carpentry workshops in August 2017 and 2018 ([https://jcoliver.github.io/2017-08-09-lc-tucson](https://jcoliver.github.io/2017-08-09-lc-tucson), [https://jcoliver.github.io/2018-08-06-lc-tucson/](https://jcoliver.github.io/2018-08-06-lc-tucson/)). These workshops have been well received at the University of Arizona and inspired a number of librarians at UAL to contribute to lesson development in the 2018 Library Carpentry Mozilla Global Sprint ([http://librarycarpentry.org/blog/2018/05/16/our-latest-sprint/](http://librarycarpentry.org/blog/2018/05/16/our-latest-sprint/)). For those librarians who posses basic computational competencies, the companion Software and Data Carpentry workshops ([http://carpentries.org](http://carpentries.org)) provide a deeper dive into research computing skills, but are still designed for primarily novice audiences. The intent of all three carpentries workshops are not necessarily to make participants proficient practitioners, but to create a community of learners empowered find the answers to data and coding questions on their own (Wilson 2016).

The short format of carpentries-style workshops provide a starting point for computational skills development, but there are few additional options that provide a deeper dive into data science topics. However, one outstanding option for in-depth skills development is the Data Science and Visualization Institute for Librarians (DSVIL) at North Carolina State University ([https://www.lib.ncsu.edu/data-science-and-visualization-institute](https://www.lib.ncsu.edu/data-science-and-visualization-institute)). This week-long course focuses on library-related applications of data science. In part an extension of the Data Science Training for Librarians course ([http://www.dst4l.info/](http://www.dst4l.info/)), the DSVIL curriculum includes data visualization, web scraping, and network analysis. Burton et al.  (2017) provide considerable discussion about the opportunities and challenges of skills development of library staff.

In some cases, current library staff may not have the time for developing deep enough skills to provide thorough data science support (Tenopir et al. 2015). In such cases, hiring additional staff can provide significant value to the type of data science support an academic library can provide. M.L.S. programs focused on data science provide one training route for data literate LIS professionals. For example, the Indiana University School of Informatics, Computing, and Engineering includes a data science specialization in the M.L.S. degree program ([https://www.sice.indiana.edu/graduate/degrees/information-library-science/dual-degrees/data-science-mls.html](https://www.sice.indiana.edu/graduate/degrees/information-library-science/dual-degrees/data-science-mls.html)). With coursework including machine learning and cloud computing, programs such as this provide the specialized training that would allow library personnel to collaborate with researchers in largely underexplored modes.

Alternatively, libraries could hire specialists with education and expertise outside of the library sciences. Such an approach has the potential to create "credentialing tension" (Burton et al. 2017), but may be especially effective for developing support in more science-oriented areas such as data analysis and visualization. Hiring non-M.L.S. professionals into library positions may also require an institutional cultural shift, as the work of data scientists is often considered the domain of other units on campus (Tenopir et al. 2015). However, it is our assertion that the expertise in quantitative skills of data-intensive research afforded by personnel without formal library training can be invaluable in supporting data science through the academic library.

# Upshot
The opportunities to engage with researchers in the realm of data science are rapidly growing. Given interest in "big data" approaches, the only question for academic libraries is "how". Assessing campus data science literacy needs can guide the mode and type of personnel development, either training existing staff or hiring new staff. The pace of change in data science also requires agility to try new services, sometimes even when those services disrupt the model of the "traditional" library. Support from library administration for data science "skunkworks", like those advocated for digital scholarship  (Nowviskie 2013), affords freedom to capitalize on new opportunities as they arise. The expanding data science ecosystem offers a new mode of engagement with campus researchers that would solidify the academic library's position as a campus hub for the data-driven twenty-first century.

# References

Aarts, Alexander A., Joanna E. Anderson, Christopher J. Anderson, Peter R. Attridge, Angela Attwood, Jordan Axt, Molly Babel, et al. "Estimating the Reproducibility of Psychological Science." Science 349, no. 6251 (2015): aa4716. doi:10.1126/science.aac4716.

Allaire, J. J., Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, and Winston Chang. Rmarkdown: Dynamic Documents for R 2018. https://CRAN.R-project.org/package=rmarkdown.

Antell, Karen, Jody Bales Foote, Jaymie Turner, and Brian Shults. "Dealing with Data: Science Librarians' Participation in Data Management at Association of Research Libraries Institutions." College \& Research Libraries 75, no. 4 (JUL, 2014): 557-574. doi:10.5860/crl.75.4.557.

Baker, J., C. Moore, E. Priego, R. Alegre, J. Cope, L. Price, O. Stephens, D. van Strien, and G. Wilson. "Library Carpentry: Software Skills Training for Library Professionals." Liber Quarterly : The Journal of European Research Libraries 26, no. 3 (2016): 141-162. doi:10.18352/lq.10176.

Barba, Lorena A. "Terminologies for Reproducible Research." CoRR abs/1802.03311, (2018). http://arxiv.org/abs/1802.03311.

Barone, Lindsay, Jason Williams, and David Micklos. "Unmet Needs for Analyzing Biological Big Data: A Survey of 704 NSF Principal Investigators." Plos Computational Biology 13, no. 10 (OCT, 2017). doi:10.1371/journal.pcbi.1005755.

Burton, Matt, Liz Lyon, Chris Erdmann, and Bonnie Tijernia. Shifting to Data Savvy: The Future of Data Science in Libraries: University of Pittsburgh, 2018. http://d-scholarship.pitt.edu/id/eprint/33891.

Camerer, Colin F., Anna Dreber, Eskil Forsell, Teck-Hua Ho, Juergen Huber, Magnus Johannesson, Michael Kirchler, et al. "Evaluating Replicability of Laboratory Experiments in Economics." Science 351, no. 6280 (2016): 1433-1436. doi:10.1126/science.aaf0918.

Carey, Maureen A. and Jason A. Papin. "Ten Simple Rules for Biologists Learning to Program." Plos Computational Biology 14, no. 1 (JAN, 2018). doi:10.1371/journal.pcbi.1005871.

Conrad, O., B. Bechtel, M. Bock, H. Dietrich, E. Fischer, L. Gerlitz, J. Wehberg, V. Wichmann, and J. Böhner. "System for Automated Geoscientific Analyses (SAGA) V. 2.1.4." Geoscientific Model Development 8, (2015): 1991-2007. doi:10.5194/gmd-8-1991-2015.

Donoho, David. "50 Years of Data Science." Journal of Computational and Graphical Statistics 26, no. 4 (2017): 745-766. doi:10.1080/10618600.2017.1384734.

Environmental Systems, Research I. "ArcGIS." . https://www.esri.com/en-us/store/arcgis-desktop.

Foundation, Python Software. Python Language Reference 2018. https://www.python.org.

Galanek, J. D. and D. C. Brooks. Supporting Faculty Research with Information Technology. Research Report: Educause Center for Analysis and Research, 2018.

Garcia-Milian, R., D. Hersey, M. Vukmirovic, and F. Duprilot. "Data Challenges of Biomedical Researchers in the Age of Omics." PeerJ 6, (2018): e5553. doi:10.7717/peerj.5553.

Huppenkothen, Daniela, Anthony Arendt, David W. Hogg, Karthik Ram, Jacob T. VanderPlas, and Ariel Rokem. "Hack Weeks as a Model for Data Science Education and Collaboration." Proceedings of the National Academy of Sciences of the United States of America 115, no. 36 (2018): 8872-8877. doi:10.1073/pnas.1717196115.

Ioannidis, J. P. A. "Why most Published Research Findings are False." Plos Medicine 2, no. 8 (2005): 696-701. doi:10.1371/journal.pmed.0020124.

Kerr, N. L. "HARKing: Hypothesizing After the Results are Known." Personality and Social Psychology Review : An Official Journal of the Society for Personality and Social Psychology, Inc 2, no. 3 (1998): 196-217. doi:10.1207/s15327957pspr0203_4.

Kluyver, Thomas, Benjamin Ragan-Kelley, Fernando Pérez, Brian Granger, Matthias Bussonnier, Jonathan Frederic, Kyle Kelley, et al. "Jupyter Notebooks -- a Publishing Format for Reproducible Computational Workflows."IOS Press, 2016. doi:10.3233/978-1-61499-649-1-87. http://ebooks.iospress.nl/publication/42900.

Knuth, D. E. "Literate Programming." Computer Journal 27, no. 2 (1984): 97-111.

Kornfield, Rachel, Prathusha K. Sarma, V. Shah Dhavan, Fiona McTavish, Gina Landucci, Klaren Pe-Romashko, and David H. Gustafson. "Detecting Recovery Problems just in Time: Application of Automated Linguistic Analysis and Supervised Machine Learning to an Online Substance Abuse Forum." Journal of Medical Internet Research 20, no. 6 (JUN, 2018). doi:10.2196/10136.

Lancaster, Jenessa, Romy Lorenz, Rob Leech, and James H. Cole. "Bayesian Optimization for Neuroimaging Pre-Processing in Brain Age Classification and Prediction." Frontiers in Aging Neuroscience 10, (FEB 12, 2018). doi:10.3389/fnagi.2018.00028.

Merchant, Nirav, Eric Lyons, Stephen Goff, Matthew Vaughn, Doreen Ware, David Micklos, and Parker Antin. "The iPlant Collaborative: Cyberinfrastructure for Enabling Data to Discovery for the Life Sciences." Plos Biology 14, no. 1 (2016): e1002342.

Moore-Sloan Data, Science Environments. Creating Institutional Change in Data Science, 2018. http://msdse.org/creating_institutional_change.html.

National Academies of Sciences, Engineering, and Medicine. Data Science for Undergraduates: Opportunities and Options. Washington, DC: The National Academies Press, 2018. doi:10.17226/25104. https://www.nap.edu/catalog/25104/data-science-for-undergraduates-opportunities-and-options.

Nowviskie, Bethany. "Skunks in the Library: A Path to Production for Scholarly R&D." Journal of Library Administration 53, no. 1 (2013): 53-66. doi:10.1080/01930826.2013.756698. https://doi.org/10.1080/01930826.2013.756698.

Oliver, Jeffrey C. "Bioinformatic Training Needs at a Health Sciences Campus." Plos One 12, no. 6 (JUN 14, 2017). doi:10.1371/journal.pone.0179581.

QGIS, Development T. "QGIS Geographic Information System." Open Source Geospatial Foundation Project. http://qgis.osgeo.org.

Rubinstein, Amir and Benny Chor. "Computational Thinking in Life Science Education." Plos Computational Biology 10, no. 11 (NOV, 2014). doi:10.1371/journal.pcbi.1003897.

Sayre, Franklin and Amy Riegelman. "The Reproducibility Crisis and Academic Libraries." College & Research Libraries 79, no. 1 (2018): 2-9. doi:10.5860/crl.79.1.2.

Spies, J. R. "The Open Science Framework: Improving Science by Making it Open and Accessible."University of Virginia, 2013. https://dl.acm.org/citation.cfm?id=2539283.

Steeves, V. "Reproducibility Librarianship." Collaborative Librarianship 9, no. 2 (2017): 4. https://digitalcommons.du.edu/collaborativelibrarianship/vol9/iss2/4.

Stingone, Jeanette A., Om P. Pandey, Luz Claudio, and Gaurav Pandey. "Using Machine Learning to Identify Air Pollution Exposure Profiles Associated with Early Cognitive Skills among US Children." Environmental Pollution 230, (NOV, 2017): 730-740. doi:10.1016/j.envpol.2017.07.023.

Team, GRASS Development. Geographic Resources Analysis Support System (GRASS GIS) Software, Version 7.2 Open Source Geospatial Foundation, 2017. http://grass.osgeo.org.

Team, R. Core. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing, 2018. https://www.R-project.org.

Tenopir, C., D. Hughes, S. Allard, M. Frame, B. Birch, L. Baird, R. Sandusky, M. Langseth, and A. Lundeen. "Research Data Services in Academic Libraries: Data Intensive Roles for the Future?" Journal of eScience Librarianship 4, no. 2 (2015): e1085. doi:10.7191/jeslib.2015.1085.

Valle, D. and A. Berdanier. "Computer Programming Skills for Environmental Sciences." Bulletin of the Ecological Society of America 93, no. 4 (2012): 373-389. doi:10.1890/0012-9623-93.4.373.

Wickham, Hadley. Ggplot2: Elegant Graphics for Data Analysis Springer-Verlag New York, 2016. http://ggplot2.org.

Wilson, G. "Software Carpentry: Lessons Learned." F1000Research 3, (2016): 24. doi:10.12688/f1000research.3-62.v2.

Xie, Yihui. "Knitr: A Comprehensive Tool for Reproducible Research in R." In Implementing Reproducible Computational Research, edited by Stodden, Victoria, Friedrich Leisch and Roger D. Peng: Chapman and Hall/CRC, 2014. http://www.crcpress.com/product/isbn/9781466561595.

Ye, Hao, Michael Brown, and Jenny Harding. "GIS for all: Exploring the Barriers and Opportunities for Underexploited GIS Applications." Nottingham, UK, 2013. doi:10.7275/R5W66J0K. https://scholarworks.umass.edu/foss4g/vol13/iss1/4.
